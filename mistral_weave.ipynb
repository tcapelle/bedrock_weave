{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83a26a16-79e4-4635-83f0-b9d39fac8282",
   "metadata": {},
   "source": [
    "# Using Weights & Biases Weave with AWS Bedrock\n",
    "\n",
    "In this notebook you will learn to use our newly realeased tool for the LLM practitioner\n",
    "\n",
    "You can use [Weave](https://wandb.github.io/weave/) to:\n",
    "\n",
    "- Log and debug language model inputs, outputs, and traces\n",
    "- Build rigorous, apples-to-apples evaluations for language model use cases\n",
    "- Organize all the information generated across the LLM workflow, from experimentation to evaluations to production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a1835b-7b58-41e3-b427-e6aefb6bfb5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ace11bef-d89e-4439-ad1d-00a603fd9453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -Uq wandb weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98b81f67-d5eb-4489-a20c-e11337d25e72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509a8001-81ec-439e-a425-6c98ffcb5c27",
   "metadata": {},
   "source": [
    "## log to your W&B account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4ab5831-7331-4ef7-a212-d618bcdded1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcapecape\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974ab8c6-191f-4ae0-a11c-d0eac506be72",
   "metadata": {},
   "source": [
    "## create a W&B project to store your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74e426e5-f60e-4333-8f4a-38ceac4af672",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as W&B user capecape.\n",
      "View Weave data at https://wandb.ai/capecape/bedrock-weave/weave\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import weave\n",
    "weave.init('bedrock-weave')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cbfcdf-780a-4b62-a235-1b32ff3b6d9f",
   "metadata": {},
   "source": [
    "Decorate your function call, that's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c64b3e2a-85ea-4514-8fe4-7d6fed2ff517",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@weave.op() # <- just add this üòé\n",
    "def generate_text(\n",
    "    model_id: str, \n",
    "    prompt: str, \n",
    "    max_tokens: int=400,\n",
    "    temperature: float=0.7,\n",
    "    top_p: float=0.7,\n",
    "    top_k: int=50,\n",
    ") -> dict:\n",
    "    bedrock = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "\n",
    "    body = json.dumps({\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "    })\n",
    "    \n",
    "    response = bedrock.invoke_model(\n",
    "        body=body,\n",
    "        modelId=model_id\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    outputs = response_body.get('outputs')\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b899140e-11b1-4dba-be2c-6319548e7e89",
   "metadata": {},
   "source": [
    "Let's first try using Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db351094-c5b7-4194-af04-b4ddc85399c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/capecape/bedrock-weave/r/call/4dff4369-4d76-45f4-a0d5-a4c80298677d\n",
      "Output 1\n",
      "----------\n",
      "Text:\n",
      " Title: Vegan Carbonara with Creamy Cashew Sauce\n",
      "\n",
      "Prep Time: 15 minutes\n",
      "Cook Time: 10 minutes\n",
      "Total Time: 25 minutes\n",
      "Servings: 4\n",
      "\n",
      "Ingredients:\n",
      "- 1 pound (450g) spaghetti or your preferred pasta\n",
      "- 1 1/2 cups (375ml) raw, unsalted cashews, soaked in water for at least 2 hours and drained\n",
      "- 1 1/2 cups (375ml) vegetable broth\n",
      "- 1/2 cup (120ml) unsweetened almond milk\n",
      "- 1/2 cup (120g) nutritional yeast\n",
      "- 2 tbsp (30ml) olive oil, divided\n",
      "- 1 large onion, finely chopped\n",
      "- 4 cloves garlic, minced\n",
      "- 1 cup (240g) cooked chickpeas or firm tofu, drained and rinsed\n",
      "- 1/2 cup (120g) steamed or roasted broccoli\n",
      "- 1/2 cup (120g) sliced mushrooms\n",
      "- 1/2 cup (120g) sliced zucchini\n",
      "- 1/2 cup (120g) chopped kale\n",
      "- Salt and freshly ground black pepper, to taste\n",
      "- 1 tbsp (15ml) lemon juice\n",
      "- 1/2 cup (120g) vegan bacon or pancetta (optional)\n",
      "- 1/4 cup (60g) chopped fresh parsley\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Cook pasta according to package instructions in a large pot of salted boiling water until al dente.\n",
      "\n",
      "Stop reason: length\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = 'mistral.mistral-7b-instruct-v0:2'\n",
    "\n",
    "prompt = \"\"\"<s>[INST] Create a Vegan Carbonara Recipe[/INST] \"\"\"\n",
    "\n",
    "outputs = generate_text(model_id, prompt)\n",
    "\n",
    "for index, output in enumerate(outputs):\n",
    "\n",
    "    print(f\"Output {index + 1}\\n----------\")\n",
    "    print(f\"Text:\\n{output['text']}\\n\")\n",
    "    print(f\"Stop reason: {output['stop_reason']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba9053-cb81-4950-aaa0-91d2f7ff552f",
   "metadata": {},
   "source": [
    "## Translation project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c98752-6596-4429-a58f-18cdaab5f98f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's use these powerful LLMs to translate the documentation of Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "974487f2-471d-4ab2-b0c3-587d97327339",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "# Instructions\n",
    "\n",
    "You are a documentation translation assistant from English to {output_language}. We are translating valid docusaurus flavored markdown. Some rules to remember:\n",
    "\n",
    "- Do not add extra blank lines.\n",
    "- The results must be valid docusaurus markdown\n",
    "- It is important to maintain the accuracy of the contents but we don't want the output to read like it's been translated. So instead of translating word by word, prioritize naturalness and ease of communication.\n",
    "- In code blocks, just translate the comments and leave the code as is.\n",
    "\n",
    "\n",
    "## Formatting Rules\n",
    "\n",
    "Do not translate target markdown links. Never translate the part of the link inside (). For instance here [https://wandb.ai/site](https://wandb.ai/site) do not translate anything, but on this, you should translate the [] part:\n",
    "[track metrics](./guides/track), [create logs](./guides/artifacts).\n",
    "\"\"\"\n",
    "human_prompt = \"\"\"\n",
    "Here is a chunk of documentation in docusaurus Markdown format to translate. Return the translation only, without adding anything else. \n",
    "<Markdown start>\n",
    "{md_chunk}\n",
    "<End of Markdown>\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class PromptTemplate:\n",
    "    system_prompt: str\n",
    "    human_prompt: str\n",
    "    language: str\n",
    "    \n",
    "    def format_mistral_instruct(self, md_chunk):\n",
    "        \"A formatting function for Mistral Instruct models\"\n",
    "        system_prompt = self.system_prompt.format(output_language=self.language)\n",
    "        human_prompt = self.human_prompt.format(md_chunk=md_chunk)\n",
    "        prompt = f\"<s>[INST] {system_prompt}\\n{human_prompt}[/INST] \"\"\"\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4f77ef2-9514-4f15-8bc6-65d0e3372a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(system_prompt, human_prompt, \"spanish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ff6fd-f9f4-4769-9811-01f462bbd404",
   "metadata": {},
   "source": [
    "let's read 2 files from our documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81557531-0135-4f44-b26c-8b7e28520cbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc = Path(\"./docs/quickstart.md\").read_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99d90b8-e64a-4973-a37d-5ef2897c9871",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can now call the model with this new prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78733554-fe65-4173-8bd0-b4f1998a196e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/capecape/bedrock-weave/r/call/e87a266d-2ddf-41ea-b38b-5ec8c310c240\n",
      "> Text:\n",
      "---\n",
      "description: Inicio r√°pido de W&B.\n",
      "displayed_sidebar: por defecto\n",
      "---\n",
      "\n",
      "import Tabs from '@theme/Tabs';\n",
      "import TabItem from '@theme/TabItem';\n",
      "\n",
      "# Inicio r√°pido\n",
      "\n",
      "Instale W&B y monitoree sus experimen\n",
      "\n",
      "> Stop reason: length\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = generate_text(model_id, prompt_template.format_mistral_instruct(doc), max_tokens=2048)\n",
    "print(f\"> Text:\\n{output[0]['text'][0:200]}\\n\")  # print first 200 chars\n",
    "print(f\"> Stop reason: {output[0]['stop_reason']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4708c4d3-4707-42d3-b59c-1335a7f55477",
   "metadata": {},
   "source": [
    "We can improve our weave experience by creating a `Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e16f5bb-05ab-44fa-9a2b-f9233231a7cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from weave import Model\n",
    "\n",
    "class MistralInstruct(Model):\n",
    "    prompt_template: PromptTemplate\n",
    "    temperature: float=0.7\n",
    "    max_tokens: int=2048\n",
    "    model_id: str='mistral.mistral-7b-instruct-v0:2'\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, document: str) -> dict:\n",
    "        output = generate_text(\n",
    "             model_id, \n",
    "             self.prompt_template.format_mistral_instruct(document), \n",
    "             temperature=self.temperature,\n",
    "             max_tokens=self.max_tokens,\n",
    "        )\n",
    "        return output[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf9ea785-c850-4780-9336-7521399e301e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MistralInstruct(prompt_template=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7285ce21-6663-4293-b819-305e2ff670cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/capecape/bedrock-weave/r/call/ed0d66aa-d021-4667-b66a-ca29cc99afdd\n"
     ]
    }
   ],
   "source": [
    "out = model.predict(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b59f3ade-72c9-46a5-ad0a-9a88f08a8c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "description: R√°pido inicio con W&B.\n",
      "displayed_sidebar: default\n",
      "---\n",
      "\n",
      "import Tabs from '@theme/Tabs';\n",
      "import TabItem from '@theme/TabItem';\n",
      "\n",
      "# R√°pido inicio\n",
      "\n",
      "Instale W&B y monitoree tus experimentos de aprendizaje autom√°tico en minutos.\n",
      "\n",
      "## 1. Crea una cuenta y instala W&B\n",
      "Antes de empezar, aseg√∫rate de crear una cuenta y de instalar W&B:\n",
      "\n",
      "1. [Reg√≠strate](https://wandb.ai/site) gratis en [https:\n"
     ]
    }
   ],
   "source": [
    "print(out[0:400])"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
